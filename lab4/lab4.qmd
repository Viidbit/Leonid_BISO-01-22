---
title: "Исследование метаданных DNS трафика"
subtitle: "Практика №4"
author: "Lona610@yandex.ru"
format: 
  md:
    output-file: README.md
---



## Цель работы

1.  Зекрепить практические навыки использования языка программирования R
    для обработки данных
2.  Закрепить знания основных функций обработки данных экосистемы
    tidyverse языка R
3.  Закрепить навыки исследования метаданных DNS трафика

## Исходные данные

1.  Программное обеспечение ОС Windows 11
2.  VS Code
3.  Интерпретатор языка R 4.5.1

## Задания

1.  Импортируйте данные DNS –
    https://storage.yandexcloud.net/dataset.ctfsec/dns.zip Данные были
    собраны с помощью сетевого анализатора zeek
2.  Добавьте пропущенные данные о структуре данных (назначении столбцов)
3.  Преобразуйте данные в столбцах в нужный формат,просмотрите общую
    структуру данных с помощью функции glimpse()
4.  Сколько участников информационного обмена всети Доброй Организации?
5.  Какое соотношение участников обмена внутрисети и участников
    обращений к внешним ресурсам?
6.  Найдите топ-10 участников сети, проявляющих наибольшую сетевую
    активность.
7.  Найдите топ-10 доменов, к которым обращаются пользователи сети и
    соответственное количество обращений
8.  Опеределите базовые статистические характеристики (функция summary()
    ) интервала времени между последовательными обращениями к топ-10
    доменам.
9.  Часто вредоносное программное обеспечение использует DNS канал в
    качестве канала управления, периодически отправляя запросы на
    подконтрольный злоумышленникам DNS сервер. По периодическим запросам
    на один и тот же домен можно выявить скрытый DNS канал. Есть ли
    такие IP адреса в исследуемом датасете?
10. Определите местоположение (страну, город) и организацию-провайдера
    для топ-10 доменов. Для этого можно использовать сторонние
    сервисы,например http://ip-api.com (API-эндпоинт –
    http://ip-api.com/json).

## Шаги:

1 Импортируйте данные DNS

``` {r}
library(tidyverse)
```



``` {r}

library(tidyverse)
test <- tempfile()
download.file("https://storage.yandexcloud.net/dataset.ctfsec/dns.zip", test)
unzip(test, exdir = "dns_data")
dns_data <- read_tsv("dns_data/dns.log", col_names = FALSE)
```

 
2 Добавьте пропущенные данные о структуре данных (назначении столбцов) 



``` {r}

colnames(dns_data) <- c("ts", "uid", "id.orig_h", "id.orig_p", "id.resp_h", "id.resp_p", "proto", "trans_id", "rtt", "query", "qclass", "qclass_name","qtype", "qtype_name", "rcode", "rcode_name", "AA", "TC", "RD", "RA", "Z", "answers", "TTLs", "rejected")
```

3 Преобразуйте данные в столбцах в нужный формат

``` {r}
dns_data <- dns_data %>% mutate(ts = as_datetime(ts), id.orig_h = as.character(id.orig_h), id.resp_h = as.character(id.resp_h), query = as.character(query))
```

4 Просмотрите общую структуру данных с помощью функции glimpse()

``` {r}
glimpse(dns_data)
```


5 Сколько участников информационного обмена в сети Доброй Организации?

``` {r}
all_ips <- unique(c(dns_data$id.orig_h, dns_data$id.resp_h))
cat("Количество участников информационного обмена:", length(all_ips), "\n")
```




6 Какое соотношение участников обмена внутри сети и участников обращений
к внешним ресурсам?

``` {r}
internal_ips <- all_ips[grepl("^(10\\.|192\\.168\\.|172\\.(1[6-9]|2[0-9]|3[0-1])\\.)", all_ips)]
external_ips <- setdiff(all_ips, internal_ips)

cat("Внутренние IP:", length(internal_ips), "\n")
cat("Внешние IP:", length(external_ips), "\n")
cat("Соотношение (внутренние/внешние):", round(length(internal_ips)/length(external_ips), 2), "\n")
```

7 Найдите топ-10 участников сети, проявляющих наибольшую сетевую
активность.

``` {r}
activity_orig <- dns_data %>% 
  count(id.orig_h, name = "requests_sent") %>% 
  rename(ip = id.orig_h)

activity_resp <- dns_data %>% 
  count(id.resp_h, name = "requests_received") %>% 
  rename(ip = id.resp_h)

total_activity <- full_join(activity_orig, activity_resp, by = "ip") %>%
  mutate(
    requests_sent = replace_na(requests_sent, 0),
    requests_received = replace_na(requests_received, 0),
    total_activity = requests_sent + requests_received
  ) %>%
  arrange(desc(total_activity)) %>%
  head(10)

cat("\nТоп-10 участников по сетевой активности:\n")
print(total_activity)
```



8 Найдите топ-10 доменов, к которым обращаются пользователи сети и
соответственное количество обращений

``` {r}
top_domains <- dns_data %>%
  count(query, sort = TRUE) %>%
  head(10)

cat("\nТоп-10 доменов по количеству обращений:\n")
print(top_domains)
```


    Топ доменов:

``` {r}
print(top_domains)
```





9 Опеределите базовые статистические характеристики (функция summary())
интервала времени между последовательными обращениями к топ-10 доменам

``` {r}
top_domain_stats <- dns_data %>%
  filter(query %in% top_domains$query) %>%
  arrange(query, ts) %>%
  group_by(query) %>%
  mutate(time_diff = as.numeric(difftime(ts, lag(ts), units = "secs"))) %>%
  summarise(
    min = min(time_diff, na.rm = TRUE),
    q1 = quantile(time_diff, 0.25, na.rm = TRUE),
    median = median(time_diff, na.rm = TRUE),
    mean = mean(time_diff, na.rm = TRUE),
    q3 = quantile(time_diff, 0.75, na.rm = TRUE),
    max = max(time_diff, na.rm = TRUE),
    .groups = 'drop'
  )

cat("\nСтатистика интервалов времени для топ-10 доменов:\n")
print(top_domain_stats)
```


10 Часто вредоносное программное обеспечение использует DNS канал в
качестве канала управления, периодически отправляя запросы на
подконтрольный злоумышленникам DNS сервер. По периодическим запросам на
один и тот же домен можно выявить скрытый DNS канал. Есть ли такие IP
адреса в исследуемом датасете?

``` {r}
suspicious_activity <- dns_data %>%
  count(id.orig_h, query, sort = TRUE) %>%  
  filter(n > 5) %>%
  head(10)

if(nrow(suspicious_activity) > 0) {
  cat("\nПодозрительные IP (частые запросы к одному домену):\n")
  print(suspicious_activity)
} else {
  cat("\nПодозрительная активность отсутствует\n")
}

```

11 Определите местоположение (страну, город) и организацию-провайдера 
для топ-10 доменов. Для этого можно использовать сторонние сервисы,
например http://ip-api.com (API-эндпоинт – http://ip-api.com/json).

``` {r}

# Импорт необходимых пакетов
library(httr)
library(jsonlite)
library(dplyr)

# Определение исследуемых доменных имен
selected_domains <- c(
  "google.com", "youtube.com", "facebook.com", 
  "baidu.com", "wikipedia.org", "qq.com", 
  "taobao.com", "yahoo.com", "amazon.com", 
  "twitter.com"
)

# Функция получения географических данных домена
fetch_geo_data <- function(domain_name) {
  api_endpoint <- sprintf("http://ip-api.com/json/%s", domain_name)
  
  attempt <- tryCatch({
    api_response <- GET(api_endpoint)
    
    if (http_status(api_response)$category == "Success") {
      response_data <- content(api_response, "parsed")
      
      tibble(
        Domain = domain_name,
        IP_address = response_data$query %||% NA,
        Country = response_data$country %||% NA,
        City = response_data$city %||% NA,
        ISP = response_data$isp %||% NA,
        Organization = response_data$org %||% NA,
        AS_number = response_data$as %||% NA,
        Request_status = response_data$status %||% NA
      )
    } else {
      message("Сбой запроса для ", domain_name, " - код: ", status_code(api_response))
      NULL
    }
  }, error = function(err) {
    message("Ошибка при обработке ", domain_name, ": ", err$message)
    NULL
  })
  
  return(attempt)
}

# Модифицированная функция с задержкой выполнения
fetch_with_delay <- function(domain) {
  result_data <- fetch_geo_data(domain)
  Sys.sleep(2.0)
  return(result_data)
}

# Основной процесс сбора данных
cat("Запуск процесса сбора географических данных...\n\n")

collected_results <- list()

for (current_domain in selected_domains) {
  cat("Анализируется: ", current_domain, "\n")
  domain_data <- fetch_with_delay(current_domain)
  
  if (!is.null(domain_data)) {
    collected_results[[length(collected_results) + 1]] <- domain_data
  }
}

# Объединение результатов
final_results <- bind_rows(collected_results)

# Вывод итоговой информации
cat("\n", strrep("=", 50), "\n")
cat("ОБРАБОТАННЫЕ ДОМЕНЫ\n")
cat(strrep("=", 50), "\n")

if (nrow(final_results) > 0) {
  # Детальный вывод по каждому домену
  cat("\nДЕТАЛЬНАЯ ИНФОРМАЦИЯ:\n")
  cat(strrep("-", 50), "\n")
  
  for (row_index in seq_len(nrow(final_results))) {
    current_row <- final_results[row_index, ]
    
    cat(sprintf("\n%2d. %s\n", row_index, current_row$Domain))
    cat("    ├─ IP-адрес: ", current_row$IP_address, "\n")
    cat("    ├─ Страна: ", current_row$Country, "\n")
    cat("    ├─ Город: ", current_row$City, "\n")
    cat("    ├─ Провайдер: ", current_row$ISP, "\n")
    cat("    └─ Организация: ", current_row$Organization, "\n")
  }
  
  # Анализ распределения по странам
  country_distribution <- final_results %>%
    group_by(Country) %>%
    summarise(Количество = n(), .groups = 'drop') %>%
    arrange(desc(Количество))
  
  cat("\n", strrep("=", 50), "\n")
  cat("ГЕОГРАФИЧЕСКОЕ РАСПРЕДЕЛЕНИЕ\n")
  cat(strrep("=", 50), "\n")
  print(country_distribution)
  
  # Сохранение результатов
  output_filename <- "geo_analysis_results.csv"
  write.csv(final_results, output_filename, row.names = FALSE)
  cat(sprintf("\nДанные сохранены в файл: %s\n", output_filename))
  
} else {
  cat("Не удалось получить данные для указанных доменов.\n")
}

cat("\nПроцесс завершен.\n")

```

## Оценка результата

В результате практической работы мы поняли как анализировать данные DNS
с помощью языка R.

## Вывод

Таким образом, мы научились, используя язык r, скачивать и анализировать
данные DNS.

